**************************************************************
(A) Classifier 1 - Base DT
DecisionTreeClassifier(max_depth=3)


(B) Confusion Matrix
[[243  34 109]
 [ 37 315  56]
 [254  73 133]]


(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.46      0.63      0.53       386
           M       0.75      0.77      0.76       408
           I       0.45      0.29      0.35       460

    accuracy                           0.55      1254
   macro avg       0.55      0.56      0.55      1254
weighted avg       0.55      0.55      0.54      1254


(E) The result after 5 times of train-test repetition:
average accuracy: 0.5266347687400319, variance: 0.00010647904377443507
average macro-average F1: 0.47624014271147547, variance: 0.0011735973306225248
average weighted-average F1: 0.4775813474659981, variance: 0.0007355542280538882
**************************************************************
(A) Classifier 2 - Top DT
GridSearchCV(estimator=DecisionTreeClassifier(max_depth=3),
             param_grid={'criterion': ['gini', 'entropy'],
                         'max_depth': [None, 3, 5],
                         'min_samples_split': [2, 5, 10]})
best parameters: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2}



(B) Confusion Matrix
[[233  41 137]
 [ 41 282  67]
 [219  90 144]]


(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.47      0.57      0.52       411
           M       0.68      0.72      0.70       390
           I       0.41      0.32      0.36       453

    accuracy                           0.53      1254
   macro avg       0.52      0.54      0.53      1254
weighted avg       0.52      0.53      0.52      1254


(E) The result after 5 times of train-test repetition:
best parameters for iteration1: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}
best parameters for iteration2: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}
best parameters for iteration3: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}
best parameters for iteration4: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2}
best parameters for iteration5: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5}
average accuracy: 0.5384370015948964, variance: 0.00021173711428055473
average macro-average F1: 0.5319331477587246, variance: 8.226836595723777e-05
average weighted-average F1: 0.5309520081236236, variance: 8.407493847219391e-05
**************************************************************
(A) Classifier 3 - Base MLP
MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 100),
              solver='sgd')

(B) Confusion Matrix
[[  0  63 304]
 [  0 337  74]
 [  0 125 351]]

(C, D) Classification Report
              precision    recall  f1-score   support

           F       0.00      0.00      0.00       367
           M       0.64      0.82      0.72       411
           I       0.48      0.74      0.58       476

    accuracy                           0.55      1254
   macro avg       0.37      0.52      0.43      1254
weighted avg       0.39      0.55      0.46      1254


Base MLP Train-Test Repetition

=== Iteration 1 === 
(A) MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 100),
              solver='sgd')

(B) Confusion Matrix
[[  2  73 317]
 [  0 318  94]
 [  1  95 354]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.67      0.01      0.01       392
           M       0.65      0.77      0.71       412
           I       0.46      0.79      0.58       450

    accuracy                           0.54      1254
   macro avg       0.59      0.52      0.43      1254
weighted avg       0.59      0.54      0.44      1254

=== Iteration 2 === 
(A) MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 100),
              solver='sgd')

(B) Confusion Matrix
[[  0  61 335]
 [  0 316  94]
 [  0  99 349]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.00      0.00      0.00       396
           M       0.66      0.77      0.71       410
           I       0.45      0.78      0.57       448

    accuracy                           0.53      1254
   macro avg       0.37      0.52      0.43      1254
weighted avg       0.38      0.53      0.44      1254

=== Iteration 3 === 
(A) MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 100),
              solver='sgd')

(B) Confusion Matrix
[[  2  70 323]
 [  0 309  77]
 [  2 102 369]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.50      0.01      0.01       395
           M       0.64      0.80      0.71       386
           I       0.48      0.78      0.59       473

    accuracy                           0.54      1254
   macro avg       0.54      0.53      0.44      1254
weighted avg       0.54      0.54      0.45      1254

=== Iteration 4 === 
(A) MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 100),
              solver='sgd')

(B) Confusion Matrix
[[  0  71 313]
 [  0 324  85]
 [  1 106 354]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.00      0.00      0.00       384
           M       0.65      0.79      0.71       409
           I       0.47      0.77      0.58       461

    accuracy                           0.54      1254
   macro avg       0.37      0.52      0.43      1254
weighted avg       0.38      0.54      0.45      1254

=== Iteration 5 === 
(A) MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 100),
              solver='sgd')

(B) Confusion Matrix
[[ 39  69 279]
 [  2 321  79]
 [ 42 100 323]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.47      0.10      0.17       387
           M       0.66      0.80      0.72       402
           I       0.47      0.69      0.56       465

    accuracy                           0.54      1254
   macro avg       0.53      0.53      0.48      1254
weighted avg       0.53      0.54      0.49      1254

(E) The result after 5 times of train-test repetition:
	The average accuracy / variance:  0.5391,  0.000025
	The average macro-average f1 score / variance:  0.4431,  0.000415
	The average weighted-average f1 score / variance:  0.4532,  0.000370


**************************************************************
(A) Classifier 4 - Top MLP
best parameters: {'activation': 'tanh', 'hidden_layer_sizes': (30, 50), 'solver': 'adam'}

(B) Confusion Matrix
[[146  46 175]
 [ 24 338  49]
 [135  90 251]]

(C, D) Classification Report
              precision    recall  f1-score   support

           F       0.48      0.40      0.43       367
           M       0.71      0.82      0.76       411
           I       0.53      0.53      0.53       476

    accuracy                           0.59      1254
   macro avg       0.57      0.58      0.58      1254
weighted avg       0.57      0.59      0.58      1254


Top MLP Train-Test Repetition

=== Iteration 1 === 
(A) Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (30, 50), 'solver': 'adam'}

(B) Confusion Matrix
[[104  60 257]
 [ 13 320  62]
 [ 82  65 291]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.52      0.25      0.34       421
           M       0.72      0.81      0.76       395
           I       0.48      0.66      0.56       438

    accuracy                           0.57      1254
   macro avg       0.57      0.57      0.55      1254
weighted avg       0.57      0.57      0.55      1254

=== Iteration 2 === 
(A) Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (10, 10, 10), 'solver': 'adam'}

(B) Confusion Matrix
[[152  39 208]
 [ 15 312  68]
 [152  84 224]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.48      0.38      0.42       399
           M       0.72      0.79      0.75       395
           I       0.45      0.49      0.47       460

    accuracy                           0.55      1254
   macro avg       0.55      0.55      0.55      1254
weighted avg       0.54      0.55      0.54      1254

=== Iteration 3 === 
(A) Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (30, 50), 'solver': 'adam'}

(B) Confusion Matrix
[[168  53 178]
 [ 31 324  50]
 [156  87 207]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.47      0.42      0.45       399
           M       0.70      0.80      0.75       405
           I       0.48      0.46      0.47       450

    accuracy                           0.56      1254
   macro avg       0.55      0.56      0.55      1254
weighted avg       0.55      0.56      0.55      1254

=== Iteration 4 === 
(A) Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (30, 50), 'solver': 'adam'}

(B) Confusion Matrix
[[134  52 199]
 [ 30 319  53]
 [146  92 229]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.43      0.35      0.39       385
           M       0.69      0.79      0.74       402
           I       0.48      0.49      0.48       467

    accuracy                           0.54      1254
   macro avg       0.53      0.54      0.54      1254
weighted avg       0.53      0.54      0.53      1254

=== Iteration 5 === 
(A) Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'solver': 'adam'}

(B) Confusion Matrix
[[122  53 231]
 [ 28 329  50]
 [116  80 245]]

(C,D) Classification Report
              precision    recall  f1-score   support

           F       0.46      0.30      0.36       406
           M       0.71      0.81      0.76       407
           I       0.47      0.56      0.51       441

    accuracy                           0.56      1254
   macro avg       0.55      0.55      0.54      1254
weighted avg       0.54      0.56      0.54      1254

(E) The result after 5 times of train-test repetition:
	The average accuracy / variance:  0.5550,  0.000080
	The average macro-average f1 score / variance:  0.5458,  0.000040
	The average weighted-average f1 score / variance:  0.5432,  0.000028


